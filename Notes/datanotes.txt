====================
Step 1: Data Collection & Pre-processing - See datacollection.py 
====================

1.1 Data Collection:
--------------------
    - Gathering historical data, from Yahoo Finance API, to be used in the model.
    
1.2 Handle Missing Data:
--------------------------
    - Addressing gaps or missing values in the dataset.
    
- Machine Learning Implications:
    - Missing values can lead to inaccurate model predictions and biases. Handling them ensures the model learns from a complete dataset.

1.3 Normalize or Scale Data:
------------------------------
    - Transforming data values to a specific range, often between 0 and 1.
    
- Machine Learning Implications:
    - Helps in speeding up the training process.
    - Ensures that all features contribute equally to the model's learning. It prevents features with larger scales from dominating the learning process.

1.4 Convert Data to Sequences:
-------------------------------
    - Organizing data into overlapping sequences for time-series prediction tasks.
    - Saving the next value in the sequence, ex. [5, 6, 7, 6.5]  becomes Xs [5, 6, 7] and Ys [6.5]
- Machine Learning Implications:
    - Sequences allow models, especially LSTMs, to recognize patterns over a series of data points rather than on individual points.
    - It aids in modeling memory and understanding temporal dependencies in time series data.

1.5 Split Data into Training and Testing Sets:
-----------------------------------------------
    - Dividing the dataset into two sets: one for training the model and the other for evaluating its performance.
    
- Machine Learning Implications:
    - Ensures the model's performance is evaluated on unseen data, giving a more realistic measure of its predictive capability.
    - Helps in identifying overfitting, where a model performs well on training data but poorly on new, unseen data.

1.6 Reshape for LSTM:
-----------------------
    - Modifying the shape of the data to fit the input requirements of LSTM models.
    
- Machine Learning Implications:
    - LSTM models expect input in a specific shape to process sequences effectively.
    - Ensures efficient and proper training of LSTM on the sequences.